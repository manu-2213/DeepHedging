{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')  # Adjust the path\n",
    "\n",
    "# Obtain relevant functions\n",
    "from src.pricing import black_scholes_price, black_scholes_delta\n",
    "from src.hedging import initialize_simulation, PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "n_paths = 100  # number of simulation paths\n",
    "S0 = np.full(n_paths, 100.0)  # initial stock price for each path\n",
    "K = 100.0\n",
    "T = 1.0\n",
    "r = 0.05\n",
    "sigma = 0.2\n",
    "steps = 10_000  # daily steps in one year\n",
    "dt = T / steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeltaApproximator(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, output_dim=1):\n",
    "        \"\"\"\n",
    "        Input features: current stock price S, time to expiry, risk-free rate r.\n",
    "        Output: hedge position (delta) for a call option (in [0,1]).\n",
    "        \"\"\"\n",
    "        super(DeltaApproximator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()  # to ensure output in [0,1] for call options.\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Critic: Estimates the state value\n",
    "# ---------------------------\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=32, output_dim=1):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HedgingEnv:\n",
    "    def __init__(self, S0, K, T, r, sigma, steps):\n",
    "        self.S0 = S0\n",
    "        self.K = K\n",
    "        self.T = T\n",
    "        self.r = r\n",
    "        self.sigma = sigma\n",
    "        self.steps = steps\n",
    "        self.dt = T / steps\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_S = self.S0\n",
    "        self.t = 0\n",
    "        self.shares_held = 0.0\n",
    "        self.initial_option_price = black_scholes_price(self.S0, self.K, self.T, self.r, self.sigma)\n",
    "        self.cash_account = self.initial_option_price\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # Observations: current stock price, time to expiry, and risk-free rate.\n",
    "        time_remaining = self.T - self.t * self.dt\n",
    "        return np.array([self.current_S, time_remaining, self.r], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Action is the new hedge (delta). The trade is the difference from the current hedge.\n",
    "        \"\"\"\n",
    "        # Compute the trade required.\n",
    "        delta_old = self.shares_held\n",
    "        delta_new = action\n",
    "        shares_to_trade = delta_new - delta_old\n",
    "        \n",
    "        # Trade at current price.\n",
    "        self.cash_account -= shares_to_trade * self.current_S\n",
    "        self.shares_held = delta_new\n",
    "        \n",
    "        # Cash grows at the risk-free rate.\n",
    "        self.cash_account *= np.exp(self.r * self.dt)\n",
    "        \n",
    "        # Evolve the stock price.\n",
    "        z = np.random.normal(0, 1)\n",
    "        self.current_S = self.current_S * np.exp((self.r - 0.5 * self.sigma**2) * self.dt +\n",
    "                                                 self.sigma * np.sqrt(self.dt) * z)\n",
    "        self.t += 1\n",
    "        done = (self.t >= self.steps)\n",
    "        obs = self._get_obs()\n",
    "        reward = 0.0\n",
    "        \n",
    "        if done:\n",
    "            # At expiry, compute the option payoff (for a call) and the hedging error.\n",
    "            option_payoff = max(0.0, self.current_S - self.K)\n",
    "            portfolio_value = self.cash_account + self.shares_held * self.current_S\n",
    "            reward = -abs(portfolio_value - option_payoff)\n",
    "        return obs, reward, done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Actor-Critic Training Function\n",
    "# ---------------------------\n",
    "def train_actor_critic(env, actor_net, critic_net, actor_optimizer, critic_optimizer, num_episodes=5000, gamma=1.0):\n",
    "    actor_net.train()\n",
    "    critic_net.train()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        \n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Convert observation to tensor.\n",
    "            state_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # shape: (1, 3)\n",
    "            \n",
    "            # Actor: get the mean delta.\n",
    "            delta_mean = actor_net(state_tensor)\n",
    "            fixed_std = 0.05  # Fixed std for exploration.\n",
    "            dist = torch.distributions.Normal(delta_mean, fixed_std)\n",
    "            action_tensor = dist.sample()\n",
    "            log_prob = dist.log_prob(action_tensor)\n",
    "            \n",
    "            # Ensure action is within [0,1].\n",
    "            action_clipped = torch.clamp(action_tensor, 0.0, 1.0)\n",
    "            action = action_clipped.item()\n",
    "            \n",
    "            # Critic: estimate the state value.\n",
    "            value = critic_net(state_tensor)\n",
    "            \n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            \n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "        \n",
    "        # Compute returns (cumulative discounted rewards).\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).unsqueeze(1)\n",
    "        values = torch.cat(values)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        \n",
    "        # Compute advantage.\n",
    "        advantages = returns - values.detach()\n",
    "        \n",
    "        # Actor loss (policy gradient with advantage).\n",
    "        actor_loss = - (log_probs * advantages).mean()\n",
    "        \n",
    "        # Critic loss (mean squared error).\n",
    "        critic_loss = nn.MSELoss()(values, returns)\n",
    "        \n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        actor_optimizer.zero_grad()\n",
    "        critic_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "        critic_optimizer.step()\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            total_reward = sum(rewards)\n",
    "            print(f\"Episode {episode}: Total Reward = {total_reward:.4f}, Actor Loss = {actor_loss.item():.4f}, Critic Loss = {critic_loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward = -18.0840, Actor Loss = 24.3440, Critic Loss = 245.3714\n",
      "Episode 50: Total Reward = -11.2909, Actor Loss = 1.0131, Critic Loss = 0.4753\n",
      "Episode 100: Total Reward = -6.1853, Actor Loss = -14.9981, Critic Loss = 92.5718\n",
      "Episode 150: Total Reward = -6.4276, Actor Loss = -15.9625, Critic Loss = 106.0560\n",
      "Episode 200: Total Reward = -9.8045, Actor Loss = -1.7162, Critic Loss = 2.6450\n",
      "Episode 250: Total Reward = -9.2938, Actor Loss = -8.4733, Critic Loss = 28.6252\n",
      "Episode 300: Total Reward = -10.3686, Actor Loss = -3.2546, Critic Loss = 4.6861\n",
      "Episode 350: Total Reward = -10.8989, Actor Loss = -0.6797, Critic Loss = 0.3150\n",
      "Episode 400: Total Reward = -9.4749, Actor Loss = 1.3298, Critic Loss = 1.1811\n",
      "Episode 450: Total Reward = -10.4518, Actor Loss = -1.9687, Critic Loss = 1.8083\n",
      "Episode 500: Total Reward = -10.5832, Actor Loss = -0.9849, Critic Loss = 0.5250\n",
      "Episode 550: Total Reward = -11.0193, Actor Loss = -2.8628, Critic Loss = 4.2810\n",
      "Episode 600: Total Reward = -9.4768, Actor Loss = -2.0849, Critic Loss = 3.1447\n",
      "Episode 650: Total Reward = -10.6229, Actor Loss = 3.5236, Critic Loss = 6.2100\n",
      "Episode 700: Total Reward = -9.9947, Actor Loss = -0.0330, Critic Loss = 0.3527\n",
      "Episode 750: Total Reward = -2.6389, Actor Loss = -14.8753, Critic Loss = 95.6077\n",
      "Episode 800: Total Reward = -12.8404, Actor Loss = -2.1993, Critic Loss = 2.7209\n",
      "Episode 850: Total Reward = -11.1291, Actor Loss = 2.2857, Critic Loss = 2.7618\n",
      "Episode 900: Total Reward = -17.8637, Actor Loss = 6.4359, Critic Loss = 18.6167\n",
      "Episode 950: Total Reward = -20.2751, Actor Loss = 10.8724, Critic Loss = 53.9690\n",
      "Episode 1000: Total Reward = -11.0947, Actor Loss = -1.2840, Critic Loss = 1.0710\n",
      "Episode 1050: Total Reward = -9.5679, Actor Loss = -1.1658, Critic Loss = 0.8667\n",
      "Episode 1100: Total Reward = -11.9809, Actor Loss = -8.8535, Critic Loss = 32.9670\n",
      "Episode 1150: Total Reward = -10.4080, Actor Loss = -2.5586, Critic Loss = 2.7562\n",
      "Episode 1200: Total Reward = -39.6151, Actor Loss = 41.7067, Critic Loss = 707.5157\n",
      "Episode 1250: Total Reward = -11.1946, Actor Loss = -0.9178, Critic Loss = 0.5757\n",
      "Episode 1300: Total Reward = -46.5084, Actor Loss = 53.6777, Critic Loss = 1103.9398\n",
      "Episode 1350: Total Reward = -15.9010, Actor Loss = 3.6402, Critic Loss = 6.4136\n",
      "Episode 1400: Total Reward = -12.8764, Actor Loss = -2.2459, Critic Loss = 2.3604\n",
      "Episode 1450: Total Reward = -11.3265, Actor Loss = -0.7930, Critic Loss = 1.7335\n",
      "Episode 1500: Total Reward = -11.5035, Actor Loss = 0.0096, Critic Loss = 0.2250\n",
      "Episode 1550: Total Reward = -10.4973, Actor Loss = -0.9489, Critic Loss = 0.8985\n",
      "Episode 1600: Total Reward = -10.9088, Actor Loss = 0.0063, Critic Loss = 0.3496\n",
      "Episode 1650: Total Reward = -11.1331, Actor Loss = 0.9818, Critic Loss = 1.6395\n",
      "Episode 1700: Total Reward = -8.2819, Actor Loss = -7.6375, Critic Loss = 22.9484\n",
      "Episode 1750: Total Reward = -10.2833, Actor Loss = -0.2671, Critic Loss = 0.3074\n",
      "Episode 1800: Total Reward = -44.1663, Actor Loss = 46.4184, Critic Loss = 929.9625\n",
      "Episode 1850: Total Reward = -9.9599, Actor Loss = -4.3127, Critic Loss = 7.6914\n",
      "Episode 1900: Total Reward = -31.9245, Actor Loss = 31.1134, Critic Loss = 398.5440\n",
      "Episode 1950: Total Reward = -11.2257, Actor Loss = -0.7286, Critic Loss = 0.5170\n",
      "Episode 2000: Total Reward = -9.1530, Actor Loss = -4.9935, Critic Loss = 11.2635\n",
      "Episode 2050: Total Reward = -10.9670, Actor Loss = 0.8949, Critic Loss = 1.5710\n",
      "Episode 2100: Total Reward = -10.7398, Actor Loss = -2.2728, Critic Loss = 2.3715\n",
      "Episode 2150: Total Reward = -10.6632, Actor Loss = -0.1595, Critic Loss = 0.2461\n",
      "Episode 2200: Total Reward = -90.9494, Actor Loss = 122.1252, Critic Loss = 5520.6675\n",
      "Episode 2250: Total Reward = -9.1145, Actor Loss = -6.0661, Critic Loss = 15.2975\n",
      "Episode 2300: Total Reward = -18.5801, Actor Loss = 8.3395, Critic Loss = 28.1476\n",
      "Episode 2350: Total Reward = -9.8055, Actor Loss = -6.1961, Critic Loss = 16.4258\n",
      "Episode 2400: Total Reward = -10.6652, Actor Loss = -1.3389, Critic Loss = 1.0831\n",
      "Episode 2450: Total Reward = -4.0561, Actor Loss = -12.9290, Critic Loss = 60.6699\n",
      "Episode 2500: Total Reward = -10.0427, Actor Loss = -1.7541, Critic Loss = 4.5300\n",
      "Episode 2550: Total Reward = -9.5610, Actor Loss = -1.4855, Critic Loss = 1.7665\n",
      "Episode 2600: Total Reward = -11.1204, Actor Loss = 1.4807, Critic Loss = 1.2152\n",
      "Episode 2650: Total Reward = -6.8874, Actor Loss = -7.7101, Critic Loss = 22.8038\n",
      "Episode 2700: Total Reward = -1.5214, Actor Loss = -16.4666, Critic Loss = 97.2358\n",
      "Episode 2750: Total Reward = -18.7820, Actor Loss = 13.4811, Critic Loss = 76.8938\n",
      "Episode 2800: Total Reward = -10.5635, Actor Loss = 2.0215, Critic Loss = 2.0626\n",
      "Episode 2850: Total Reward = -0.8313, Actor Loss = -20.2718, Critic Loss = 147.7512\n",
      "Episode 2900: Total Reward = -11.0294, Actor Loss = -1.4035, Critic Loss = 1.3639\n",
      "Episode 2950: Total Reward = -10.5876, Actor Loss = -1.5455, Critic Loss = 1.2389\n",
      "Episode 3000: Total Reward = -9.9161, Actor Loss = 0.4705, Critic Loss = 0.5163\n",
      "Episode 3050: Total Reward = -10.7081, Actor Loss = -1.7590, Critic Loss = 3.2375\n",
      "Episode 3100: Total Reward = -34.2381, Actor Loss = 22.9095, Critic Loss = 231.8694\n",
      "Episode 3150: Total Reward = -4.4350, Actor Loss = -11.5756, Critic Loss = 52.0087\n",
      "Episode 3200: Total Reward = -10.3402, Actor Loss = -2.0120, Critic Loss = 1.8003\n",
      "Episode 3250: Total Reward = -0.2127, Actor Loss = -18.2263, Critic Loss = 132.8514\n",
      "Episode 3300: Total Reward = -52.5620, Actor Loss = 66.1292, Critic Loss = 1693.8036\n",
      "Episode 3350: Total Reward = -8.0884, Actor Loss = -4.2977, Critic Loss = 7.6209\n",
      "Episode 3400: Total Reward = -16.6874, Actor Loss = 4.9410, Critic Loss = 12.8991\n",
      "Episode 3450: Total Reward = -17.6121, Actor Loss = 9.5608, Critic Loss = 35.0976\n",
      "Episode 3500: Total Reward = -11.6968, Actor Loss = -2.2500, Critic Loss = 2.7505\n",
      "Episode 3550: Total Reward = -10.5385, Actor Loss = -1.2714, Critic Loss = 1.0433\n",
      "Episode 3600: Total Reward = -11.0816, Actor Loss = 0.0143, Critic Loss = 0.9013\n",
      "Episode 3650: Total Reward = -5.8213, Actor Loss = -5.7931, Critic Loss = 13.9735\n",
      "Episode 3700: Total Reward = -2.8411, Actor Loss = -14.3921, Critic Loss = 76.8627\n",
      "Episode 3750: Total Reward = -10.9936, Actor Loss = -1.8999, Critic Loss = 1.9841\n",
      "Episode 3800: Total Reward = -11.7572, Actor Loss = -0.2424, Critic Loss = 1.2995\n",
      "Episode 3850: Total Reward = -10.1681, Actor Loss = -3.5389, Critic Loss = 5.7151\n",
      "Episode 3900: Total Reward = -10.5829, Actor Loss = -0.6273, Critic Loss = 0.4210\n",
      "Episode 3950: Total Reward = -9.8520, Actor Loss = -0.1294, Critic Loss = 0.5451\n",
      "Episode 4000: Total Reward = -11.1713, Actor Loss = 2.3587, Critic Loss = 2.7412\n",
      "Episode 4050: Total Reward = -21.7451, Actor Loss = 15.4827, Critic Loss = 94.6930\n",
      "Episode 4100: Total Reward = -10.3518, Actor Loss = -2.6410, Critic Loss = 3.1359\n",
      "Episode 4150: Total Reward = -10.4034, Actor Loss = -0.1715, Critic Loss = 1.0922\n",
      "Episode 4200: Total Reward = -8.3948, Actor Loss = -3.7512, Critic Loss = 5.4871\n",
      "Episode 4250: Total Reward = -26.6088, Actor Loss = 22.2488, Critic Loss = 220.3641\n",
      "Episode 4300: Total Reward = -17.9693, Actor Loss = 8.0308, Critic Loss = 25.5954\n",
      "Episode 4350: Total Reward = -10.9120, Actor Loss = 1.4213, Critic Loss = 1.3531\n",
      "Episode 4400: Total Reward = -7.1000, Actor Loss = -6.5507, Critic Loss = 16.7777\n",
      "Episode 4450: Total Reward = -1.1715, Actor Loss = -16.3491, Critic Loss = 98.3143\n",
      "Episode 4500: Total Reward = -10.7634, Actor Loss = 1.9467, Critic Loss = 2.0290\n",
      "Episode 4550: Total Reward = -10.4086, Actor Loss = 3.4023, Critic Loss = 6.3688\n",
      "Episode 4600: Total Reward = -9.9928, Actor Loss = 0.2866, Critic Loss = 0.2639\n",
      "Episode 4650: Total Reward = -2.3200, Actor Loss = -12.5038, Critic Loss = 59.3870\n",
      "Episode 4700: Total Reward = -10.3921, Actor Loss = -0.4900, Critic Loss = 0.6777\n",
      "Episode 4750: Total Reward = -11.3263, Actor Loss = 3.6160, Critic Loss = 5.8507\n",
      "Episode 4800: Total Reward = -8.1285, Actor Loss = -7.7419, Critic Loss = 24.2316\n",
      "Episode 4850: Total Reward = -1.8461, Actor Loss = -13.9516, Critic Loss = 77.4642\n",
      "Episode 4900: Total Reward = -9.1839, Actor Loss = 0.1895, Critic Loss = 0.3181\n",
      "Episode 4950: Total Reward = -10.4775, Actor Loss = -0.7153, Critic Loss = 0.3599\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Environment parameters.\n",
    "    S0 = 100.0\n",
    "    K = 100.0\n",
    "    T = 1.0\n",
    "    r = 0.05\n",
    "    sigma = 0.2\n",
    "    steps = 252  # daily hedging.\n",
    "    \n",
    "    env = HedgingEnv(S0, K, T, r, sigma, steps)\n",
    "    \n",
    "    # Instantiate actor (policy) and critic networks.\n",
    "    actor_net = DeltaApproximator(input_dim=3, hidden_dim=32, output_dim=1)\n",
    "    critic_net = ValueNetwork(input_dim=3, hidden_dim=32, output_dim=1)\n",
    "    \n",
    "    # Define separate optimizers.\n",
    "    actor_optimizer = optim.Adam(actor_net.parameters(), lr=1e-3)\n",
    "    critic_optimizer = optim.Adam(critic_net.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Train using the actor-critic method.\n",
    "    train_actor_critic(env, actor_net, critic_net, actor_optimizer, critic_optimizer, num_episodes=5000, gamma=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Total Reward = -1.3574\n",
      "Episode 50: Total Reward = -10.2214\n",
      "Episode 100: Total Reward = -12.0360\n",
      "Episode 150: Total Reward = -10.5291\n",
      "Episode 200: Total Reward = -11.0444\n",
      "Episode 250: Total Reward = -6.5201\n",
      "Episode 300: Total Reward = -10.7951\n",
      "Episode 350: Total Reward = -9.1607\n",
      "Episode 400: Total Reward = -10.0869\n",
      "Episode 450: Total Reward = -6.8954\n",
      "Episode 500: Total Reward = -10.4611\n",
      "Episode 550: Total Reward = -1.2182\n",
      "Episode 600: Total Reward = -10.2305\n",
      "Episode 650: Total Reward = -4.3425\n",
      "Episode 700: Total Reward = -0.9316\n",
      "Episode 750: Total Reward = -49.5558\n",
      "Episode 800: Total Reward = -12.5141\n",
      "Episode 850: Total Reward = -11.4693\n",
      "Episode 900: Total Reward = -11.4853\n",
      "Episode 950: Total Reward = -11.7442\n",
      "Episode 1000: Total Reward = -34.4901\n",
      "Episode 1050: Total Reward = -1.8881\n",
      "Episode 1100: Total Reward = -11.9435\n",
      "Episode 1150: Total Reward = -6.0925\n",
      "Episode 1200: Total Reward = -10.5355\n",
      "Episode 1250: Total Reward = -5.7720\n",
      "Episode 1300: Total Reward = -10.3901\n",
      "Episode 1350: Total Reward = -22.0046\n",
      "Episode 1400: Total Reward = -11.9312\n",
      "Episode 1450: Total Reward = -15.0973\n",
      "Episode 1500: Total Reward = -0.8550\n",
      "Episode 1550: Total Reward = -6.3247\n",
      "Episode 1600: Total Reward = -11.9283\n",
      "Episode 1650: Total Reward = -21.3363\n",
      "Episode 1700: Total Reward = -10.2284\n",
      "Episode 1750: Total Reward = -11.1190\n",
      "Episode 1800: Total Reward = -6.2030\n",
      "Episode 1850: Total Reward = -0.8619\n",
      "Episode 1900: Total Reward = -10.9073\n",
      "Episode 1950: Total Reward = -11.0177\n",
      "Episode 2000: Total Reward = -12.6517\n",
      "Episode 2050: Total Reward = -8.8236\n",
      "Episode 2100: Total Reward = -18.3072\n",
      "Episode 2150: Total Reward = -5.5059\n",
      "Episode 2200: Total Reward = -42.3432\n",
      "Episode 2250: Total Reward = -10.0434\n",
      "Episode 2300: Total Reward = -12.6866\n",
      "Episode 2350: Total Reward = -12.9031\n",
      "Episode 2400: Total Reward = -57.0203\n",
      "Episode 2450: Total Reward = -10.1212\n",
      "Episode 2500: Total Reward = -12.3985\n",
      "Episode 2550: Total Reward = -27.1820\n",
      "Episode 2600: Total Reward = -23.0451\n",
      "Episode 2650: Total Reward = -10.3212\n",
      "Episode 2700: Total Reward = -9.9827\n",
      "Episode 2750: Total Reward = -8.7669\n",
      "Episode 2800: Total Reward = -10.1357\n",
      "Episode 2850: Total Reward = -11.4772\n",
      "Episode 2900: Total Reward = -4.2410\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Train the policy using the REINFORCE algorithm.\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrain_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# After training, you can integrate the trained network into your dynamic hedging simulation.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# For example, pass policy_net as the neural_net argument to dynamic_delta_hedging.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m, in \u001b[0;36mtrain_policy\u001b[0;34m(env, policy_net, optimizer, num_episodes, gamma)\u001b[0m\n\u001b[1;32m     23\u001b[0m fixed_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\n\u001b[1;32m     24\u001b[0m dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(delta, fixed_std)\n\u001b[0;32m---> 25\u001b[0m action_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m action \u001b[38;5;241m=\u001b[39m action_tensor\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Save log prob for policy gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/distributions/normal.py:73\u001b[0m, in \u001b[0;36mNormal.sample\u001b[0;34m(self, sample_shape)\u001b[0m\n\u001b[1;32m     71\u001b[0m shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended_shape(sample_shape)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Example usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Environment parameters\n",
    "    S0 = 100.0\n",
    "    K = 100.0\n",
    "    T = 1.0\n",
    "    r = 0.05\n",
    "    sigma = 0.2\n",
    "    steps = 252  # daily hedging\n",
    "    \n",
    "    # Create the hedging environment.\n",
    "    env = HedgingEnv(S0, K, T, r, sigma, steps)\n",
    "    \n",
    "    # Instantiate the policy network.\n",
    "    policy_net = DeltaApproximator(input_dim=3, hidden_dim=64, output_dim=1)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train the policy using the REINFORCE algorithm.\n",
    "    train_policy(env, policy_net, optimizer, num_episodes=100000)\n",
    "    \n",
    "    # After training, you can integrate the trained network into your dynamic hedging simulation.\n",
    "    # For example, pass policy_net as the neural_net argument to dynamic_delta_hedging."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

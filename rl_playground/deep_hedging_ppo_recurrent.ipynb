{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from deep_hedging_env import HedgingEnv\n",
        "from logit_normal import LogitNormal\n",
        "from plot_utils import plot_portfolio_vs_option_price\n",
        "from torchrl.envs import GymWrapper\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data.replay_buffers import ReplayBuffer\n",
        "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
        "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
        "from torchrl.modules import ProbabilisticActor, SafeModule\n",
        "from tensordict.nn import TensorDictModule\n",
        "from torchrl.objectives import ClipPPOLoss\n",
        "from torchrl.modules import ProbabilisticActor, SafeModule\n",
        "from torchrl.modules import (\n",
        "    ValueOperator,\n",
        "    ActorValueOperator,\n",
        "    NormalParamExtractor,\n",
        ")\n",
        "from torchrl.objectives.value import GAE\n",
        "from torchrl.envs.utils import ExplorationType, set_exploration_type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPO (Recurrent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "S0 = np.array([50.0, 100.0, 200.0])\n",
        "K = np.array([[45.0, 55.0], [90.0, 110.0], [180.0, 220.0]])\n",
        "sigma = np.array([0.15, 0.2, 0.25])\n",
        "r = 0.05\n",
        "num_simulation = 32\n",
        "num_step = 250\n",
        "history_len = 5\n",
        "base_env = HedgingEnv(S0, K, sigma, r, num_simulation=num_simulation, num_step=num_step, history_len=history_len)\n",
        "env = GymWrapper(base_env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(48000, 4800)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "frames_per_batch = env.num_envs * num_step\n",
        "sub_batch_num = 10\n",
        "sub_batch_size = frames_per_batch // sub_batch_num\n",
        "frames_per_batch, sub_batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Param for PPO\n",
        "clip_param = 0.2\n",
        "value_coef = 0.5\n",
        "entropy_coef = 0.01\n",
        "# Param for GAE\n",
        "gamma = 0.99\n",
        "lmbda = 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.rnn = nn.GRU(\n",
        "            input_size=11, hidden_size=64, num_layers=2, batch_first=True, dropout=0.0\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) > 3:  # Handle 4D input\n",
        "            x_reshaped = x.view(-1, x.shape[-2], x.shape[-1])\n",
        "            output, _ = self.rnn(x_reshaped)\n",
        "            # Reshape output back to original batch dimensions\n",
        "            output = output.view(\n",
        "                x.shape[0], x.shape[1], x.shape[2], output.shape[-1]\n",
        "            )\n",
        "        else:  # Handle 3D input directly\n",
        "            output, _ = self.rnn(x)\n",
        "        output = output[..., -1, :]  # Take output from the last time step\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=11, hidden_size=64, num_layers=2, batch_first=True, dropout=0.0\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) > 3:  # Handle 4D input\n",
        "            x_reshaped = x.view(-1, x.shape[-2], x.shape[-1])\n",
        "            output, _ = self.rnn(x_reshaped)\n",
        "            # Reshape output back to original batch dimensions\n",
        "            output = output.view(\n",
        "                x.shape[0], x.shape[1], x.shape[2], output.shape[-1]\n",
        "            )\n",
        "        else:  # Handle 3D input directly\n",
        "            output, _ = self.rnn(x)\n",
        "        output = output[..., -1, :]  # Take output from the last time step\n",
        "        return output\n",
        "\n",
        "\n",
        "feature_extractor = SafeModule(\n",
        "    module=FeatureExtractor(),\n",
        "    in_keys=[\"observation\"],\n",
        "    out_keys=[\"feature\"],\n",
        ")\n",
        "policy_network = TensorDictModule(\n",
        "    nn.Sequential(torch.nn.Linear(64, 2), NormalParamExtractor()),\n",
        "    in_keys=[\"feature\"],\n",
        "    out_keys=[\"loc\", \"scale\"],\n",
        ")\n",
        "actor = ProbabilisticActor(\n",
        "    module=policy_network,\n",
        "    in_keys=[\"loc\", \"scale\"],\n",
        "    out_keys=[\"action\"],\n",
        "    distribution_class=LogitNormal,\n",
        "    return_log_prob=True,\n",
        ")\n",
        "critic = ValueOperator(\n",
        "    module=nn.Sequential(torch.nn.Linear(64, 8), nn.Tanh(), nn.Linear(8, 1)),\n",
        "    in_keys=[\"feature\"],\n",
        "    out_keys=[\"state_value\"],\n",
        ")\n",
        "model = ActorValueOperator(feature_extractor, actor, critic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ActorValueOperator(\n",
              "    module=ModuleList(\n",
              "      (0): SafeModule(\n",
              "          module=FeatureExtractor(\n",
              "            (rnn): LSTM(11, 64, num_layers=2, batch_first=True)\n",
              "          ),\n",
              "          device=cuda:0,\n",
              "          in_keys=['observation'],\n",
              "          out_keys=['feature'])\n",
              "      (1): ProbabilisticActor(\n",
              "          module=ModuleList(\n",
              "            (0): TensorDictModule(\n",
              "                module=Sequential(\n",
              "                  (0): Linear(in_features=64, out_features=2, bias=True)\n",
              "                  (1): NormalParamExtractor(\n",
              "                    (scale_mapping): biased_softplus()\n",
              "                  )\n",
              "                ),\n",
              "                device=cuda:0,\n",
              "                in_keys=['feature'],\n",
              "                out_keys=['loc', 'scale'])\n",
              "            (1): SafeProbabilisticModule()\n",
              "          ),\n",
              "          device=cuda:0,\n",
              "          in_keys=['feature'],\n",
              "          out_keys=['loc', 'scale', 'action', 'sample_log_prob'])\n",
              "      (2): ValueOperator(\n",
              "          module=Sequential(\n",
              "            (0): Linear(in_features=64, out_features=8, bias=True)\n",
              "            (1): Tanh()\n",
              "            (2): Linear(in_features=8, out_features=1, bias=True)\n",
              "          ),\n",
              "          device=cuda:0,\n",
              "          in_keys=['feature'],\n",
              "          out_keys=['state_value'])\n",
              "    ),\n",
              "    device=cuda:0,\n",
              "    in_keys=['observation'],\n",
              "    out_keys=['feature', 'loc', 'scale', 'action', 'sample_log_prob', 'state_value'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "advantage_module = GAE(\n",
        "    gamma=gamma,\n",
        "    lmbda=lmbda,\n",
        "    value_network=model.get_value_operator(),\n",
        "    shifted=True # make sure use this one for RNN\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_module = ClipPPOLoss(\n",
        "    actor_network=model.get_policy_operator(),\n",
        "    critic_network=model.get_value_operator(),\n",
        "    clip_epsilon=clip_param,\n",
        "    entropy_coef=entropy_coef,\n",
        "    value_coef=value_coef,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "optim = torch.optim.Adam(loss_module.parameters(),lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 50\n",
        "num_episodes = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50, Episode 10/200, Loss: 150.07315063476562, Avg. Reward: -4.582630157470703\n",
            "Epoch 1/50, Episode 20/200, Loss: 144.41307067871094, Avg. Reward: -4.500133514404297\n",
            "Epoch 1/50, Episode 30/200, Loss: 148.0863800048828, Avg. Reward: -4.487645626068115\n",
            "Epoch 1/50, Episode 40/200, Loss: 130.9796600341797, Avg. Reward: -4.166134834289551\n",
            "Epoch 1/50, Episode 50/200, Loss: 141.50942993164062, Avg. Reward: -4.3846869468688965\n",
            "Epoch 1/50, Episode 60/200, Loss: 149.19015502929688, Avg. Reward: -4.507399082183838\n",
            "Epoch 1/50, Episode 70/200, Loss: 150.46881103515625, Avg. Reward: -4.559741020202637\n",
            "Epoch 1/50, Episode 80/200, Loss: 168.5126190185547, Avg. Reward: -5.078275680541992\n",
            "Epoch 1/50, Episode 90/200, Loss: 155.8957977294922, Avg. Reward: -4.807435989379883\n",
            "Epoch 1/50, Episode 100/200, Loss: 146.28187561035156, Avg. Reward: -4.433305740356445\n",
            "Epoch 1/50, Episode 110/200, Loss: 156.56600952148438, Avg. Reward: -4.754453182220459\n",
            "Epoch 1/50, Episode 120/200, Loss: 150.64706420898438, Avg. Reward: -4.567797660827637\n",
            "Epoch 1/50, Episode 130/200, Loss: 153.08006286621094, Avg. Reward: -4.720951557159424\n",
            "Epoch 1/50, Episode 140/200, Loss: 155.75257873535156, Avg. Reward: -4.7388386726379395\n",
            "Epoch 1/50, Episode 150/200, Loss: 141.64756774902344, Avg. Reward: -4.343913555145264\n",
            "Epoch 1/50, Episode 160/200, Loss: 142.03224182128906, Avg. Reward: -4.378662586212158\n",
            "Epoch 1/50, Episode 170/200, Loss: 147.23777770996094, Avg. Reward: -4.451054096221924\n",
            "Epoch 1/50, Episode 180/200, Loss: 137.19508361816406, Avg. Reward: -4.166592121124268\n",
            "Epoch 1/50, Episode 190/200, Loss: 134.00091552734375, Avg. Reward: -4.152370929718018\n",
            "Epoch 1/50, Episode 200/200, Loss: 131.9486541748047, Avg. Reward: -4.049604892730713\n",
            "Epoch 2/50, Episode 10/200, Loss: 147.4079132080078, Avg. Reward: -4.508245468139648\n",
            "Epoch 2/50, Episode 20/200, Loss: 143.0053253173828, Avg. Reward: -4.331747531890869\n",
            "Epoch 2/50, Episode 30/200, Loss: 144.7419891357422, Avg. Reward: -4.2526535987854\n",
            "Epoch 2/50, Episode 40/200, Loss: 133.2571563720703, Avg. Reward: -4.1203293800354\n",
            "Epoch 2/50, Episode 50/200, Loss: 133.893310546875, Avg. Reward: -4.181005001068115\n",
            "Epoch 2/50, Episode 60/200, Loss: 139.5479736328125, Avg. Reward: -4.269572734832764\n",
            "Epoch 2/50, Episode 70/200, Loss: 132.8538360595703, Avg. Reward: -4.080685138702393\n",
            "Epoch 2/50, Episode 80/200, Loss: 145.43801879882812, Avg. Reward: -4.3753204345703125\n",
            "Epoch 2/50, Episode 90/200, Loss: 135.80494689941406, Avg. Reward: -4.223059177398682\n",
            "Epoch 2/50, Episode 100/200, Loss: 139.19342041015625, Avg. Reward: -4.301026821136475\n",
            "Epoch 2/50, Episode 110/200, Loss: 124.70549774169922, Avg. Reward: -3.911282777786255\n",
            "Epoch 2/50, Episode 120/200, Loss: 123.66600799560547, Avg. Reward: -3.7970781326293945\n",
            "Epoch 2/50, Episode 130/200, Loss: 122.4843978881836, Avg. Reward: -3.8030009269714355\n",
            "Epoch 2/50, Episode 140/200, Loss: 121.84605407714844, Avg. Reward: -3.749720573425293\n",
            "Epoch 2/50, Episode 150/200, Loss: 123.83296203613281, Avg. Reward: -3.8577656745910645\n",
            "Epoch 2/50, Episode 160/200, Loss: 131.11404418945312, Avg. Reward: -4.144520282745361\n",
            "Epoch 2/50, Episode 170/200, Loss: 141.12071228027344, Avg. Reward: -4.3304338455200195\n",
            "Epoch 2/50, Episode 180/200, Loss: 134.830322265625, Avg. Reward: -4.187763214111328\n",
            "Epoch 2/50, Episode 190/200, Loss: 132.60914611816406, Avg. Reward: -4.139297008514404\n",
            "Epoch 2/50, Episode 200/200, Loss: 137.38519287109375, Avg. Reward: -4.198271751403809\n",
            "Epoch 3/50, Episode 10/200, Loss: 151.42947387695312, Avg. Reward: -4.551492214202881\n",
            "Epoch 3/50, Episode 20/200, Loss: 156.8541259765625, Avg. Reward: -4.81706428527832\n",
            "Epoch 3/50, Episode 30/200, Loss: 137.47802734375, Avg. Reward: -4.404766082763672\n",
            "Epoch 3/50, Episode 40/200, Loss: 146.28880310058594, Avg. Reward: -4.44149923324585\n",
            "Epoch 3/50, Episode 50/200, Loss: 139.7848358154297, Avg. Reward: -4.316888809204102\n",
            "Epoch 3/50, Episode 60/200, Loss: 148.80145263671875, Avg. Reward: -4.650198936462402\n",
            "Epoch 3/50, Episode 70/200, Loss: 142.07752990722656, Avg. Reward: -4.397512435913086\n",
            "Epoch 3/50, Episode 80/200, Loss: 142.8138885498047, Avg. Reward: -4.49027681350708\n",
            "Epoch 3/50, Episode 90/200, Loss: 144.38967895507812, Avg. Reward: -4.53083610534668\n",
            "Epoch 3/50, Episode 100/200, Loss: 149.07496643066406, Avg. Reward: -4.631532669067383\n",
            "Epoch 3/50, Episode 110/200, Loss: 155.3244171142578, Avg. Reward: -4.718639850616455\n",
            "Epoch 3/50, Episode 120/200, Loss: 145.28805541992188, Avg. Reward: -4.558799743652344\n",
            "Epoch 3/50, Episode 130/200, Loss: 145.09971618652344, Avg. Reward: -4.554604530334473\n",
            "Epoch 3/50, Episode 140/200, Loss: 144.1510467529297, Avg. Reward: -4.351082801818848\n",
            "Epoch 3/50, Episode 150/200, Loss: 153.68898010253906, Avg. Reward: -4.724981784820557\n",
            "Epoch 3/50, Episode 160/200, Loss: 147.68243408203125, Avg. Reward: -4.690736770629883\n",
            "Epoch 3/50, Episode 170/200, Loss: 140.1097869873047, Avg. Reward: -4.42578649520874\n",
            "Epoch 3/50, Episode 180/200, Loss: 153.66094970703125, Avg. Reward: -4.780014991760254\n",
            "Epoch 3/50, Episode 190/200, Loss: 150.994140625, Avg. Reward: -4.716763019561768\n",
            "Epoch 3/50, Episode 200/200, Loss: 156.5600128173828, Avg. Reward: -4.86411714553833\n",
            "Epoch 4/50, Episode 10/200, Loss: 130.37965393066406, Avg. Reward: -3.9855430126190186\n",
            "Epoch 4/50, Episode 20/200, Loss: 125.0838851928711, Avg. Reward: -3.880242109298706\n",
            "Epoch 4/50, Episode 30/200, Loss: 125.58734130859375, Avg. Reward: -3.906531810760498\n",
            "Epoch 4/50, Episode 40/200, Loss: 125.6341323852539, Avg. Reward: -3.9064271450042725\n",
            "Epoch 4/50, Episode 50/200, Loss: 121.8230972290039, Avg. Reward: -3.798394203186035\n",
            "Epoch 4/50, Episode 60/200, Loss: 115.89579010009766, Avg. Reward: -3.570443868637085\n",
            "Epoch 4/50, Episode 70/200, Loss: 117.61669158935547, Avg. Reward: -3.638495445251465\n",
            "Epoch 4/50, Episode 80/200, Loss: 122.9612808227539, Avg. Reward: -3.804553985595703\n",
            "Epoch 4/50, Episode 90/200, Loss: 123.52478790283203, Avg. Reward: -3.8884499073028564\n",
            "Epoch 4/50, Episode 100/200, Loss: 116.9051284790039, Avg. Reward: -3.6711907386779785\n",
            "Epoch 4/50, Episode 110/200, Loss: 118.68000793457031, Avg. Reward: -3.7443153858184814\n",
            "Epoch 4/50, Episode 120/200, Loss: 122.94303894042969, Avg. Reward: -3.9558541774749756\n",
            "Epoch 4/50, Episode 130/200, Loss: 118.96340942382812, Avg. Reward: -3.6811575889587402\n",
            "Epoch 4/50, Episode 140/200, Loss: 119.59136962890625, Avg. Reward: -3.6815578937530518\n",
            "Epoch 4/50, Episode 150/200, Loss: 120.48059844970703, Avg. Reward: -3.718143939971924\n",
            "Epoch 4/50, Episode 160/200, Loss: 114.3429183959961, Avg. Reward: -3.658898115158081\n",
            "Epoch 4/50, Episode 170/200, Loss: 117.84058380126953, Avg. Reward: -3.7281200885772705\n",
            "Epoch 4/50, Episode 180/200, Loss: 125.39154052734375, Avg. Reward: -3.9162681102752686\n",
            "Epoch 4/50, Episode 190/200, Loss: 117.56185913085938, Avg. Reward: -3.6410343647003174\n",
            "Epoch 4/50, Episode 200/200, Loss: 128.24038696289062, Avg. Reward: -4.025609493255615\n",
            "Epoch 5/50, Episode 10/200, Loss: 126.2417221069336, Avg. Reward: -3.9163341522216797\n",
            "Epoch 5/50, Episode 20/200, Loss: 107.86151123046875, Avg. Reward: -3.4738664627075195\n",
            "Epoch 5/50, Episode 30/200, Loss: 118.71927642822266, Avg. Reward: -3.630795955657959\n",
            "Epoch 5/50, Episode 40/200, Loss: 119.83476257324219, Avg. Reward: -3.726069927215576\n",
            "Epoch 5/50, Episode 50/200, Loss: 119.33446502685547, Avg. Reward: -3.762829303741455\n",
            "Epoch 5/50, Episode 60/200, Loss: 112.35750579833984, Avg. Reward: -3.536846876144409\n",
            "Epoch 5/50, Episode 70/200, Loss: 114.60698699951172, Avg. Reward: -3.7204554080963135\n",
            "Epoch 5/50, Episode 80/200, Loss: 118.72328186035156, Avg. Reward: -3.61826491355896\n",
            "Epoch 5/50, Episode 90/200, Loss: 116.5204086303711, Avg. Reward: -3.605445384979248\n",
            "Epoch 5/50, Episode 100/200, Loss: 120.85297393798828, Avg. Reward: -3.850456953048706\n",
            "Epoch 5/50, Episode 110/200, Loss: 119.72334289550781, Avg. Reward: -3.738175630569458\n",
            "Epoch 5/50, Episode 120/200, Loss: 117.81439971923828, Avg. Reward: -3.9003143310546875\n",
            "Epoch 5/50, Episode 130/200, Loss: 116.88245391845703, Avg. Reward: -3.6278302669525146\n",
            "Epoch 5/50, Episode 140/200, Loss: 121.07926940917969, Avg. Reward: -3.8759090900421143\n",
            "Epoch 5/50, Episode 150/200, Loss: 115.50762176513672, Avg. Reward: -3.5946919918060303\n",
            "Epoch 5/50, Episode 160/200, Loss: 114.49775695800781, Avg. Reward: -3.6239287853240967\n",
            "Epoch 5/50, Episode 170/200, Loss: 111.95337677001953, Avg. Reward: -3.7342114448547363\n",
            "Epoch 5/50, Episode 180/200, Loss: 130.308837890625, Avg. Reward: -3.982481002807617\n",
            "Epoch 5/50, Episode 190/200, Loss: 113.94842529296875, Avg. Reward: -3.67012095451355\n",
            "Epoch 5/50, Episode 200/200, Loss: 120.76309967041016, Avg. Reward: -3.7314577102661133\n",
            "Epoch 6/50, Episode 10/200, Loss: 111.45104217529297, Avg. Reward: -3.6057186126708984\n",
            "Epoch 6/50, Episode 20/200, Loss: 119.48457336425781, Avg. Reward: -3.651732921600342\n",
            "Epoch 6/50, Episode 30/200, Loss: 108.17639923095703, Avg. Reward: -3.406477928161621\n",
            "Epoch 6/50, Episode 40/200, Loss: 108.39933013916016, Avg. Reward: -3.413492441177368\n",
            "Epoch 6/50, Episode 50/200, Loss: 99.6888656616211, Avg. Reward: -3.1742944717407227\n",
            "Epoch 6/50, Episode 60/200, Loss: 100.19979095458984, Avg. Reward: -3.207562208175659\n",
            "Epoch 6/50, Episode 70/200, Loss: 104.75459289550781, Avg. Reward: -3.333432674407959\n",
            "Epoch 6/50, Episode 80/200, Loss: 104.66423034667969, Avg. Reward: -3.3077127933502197\n",
            "Epoch 6/50, Episode 90/200, Loss: 105.48794555664062, Avg. Reward: -3.2750768661499023\n",
            "Epoch 6/50, Episode 100/200, Loss: 96.18235778808594, Avg. Reward: -3.1206328868865967\n",
            "Epoch 6/50, Episode 110/200, Loss: 102.68465423583984, Avg. Reward: -3.3028411865234375\n",
            "Epoch 6/50, Episode 120/200, Loss: 110.11760711669922, Avg. Reward: -3.4774889945983887\n",
            "Epoch 6/50, Episode 130/200, Loss: 114.35719299316406, Avg. Reward: -3.618602752685547\n",
            "Epoch 6/50, Episode 140/200, Loss: 107.35792541503906, Avg. Reward: -3.4135468006134033\n",
            "Epoch 6/50, Episode 150/200, Loss: 106.9580307006836, Avg. Reward: -3.3228704929351807\n",
            "Epoch 6/50, Episode 160/200, Loss: 106.76229858398438, Avg. Reward: -3.2456088066101074\n",
            "Epoch 6/50, Episode 170/200, Loss: 104.00459289550781, Avg. Reward: -3.293647050857544\n",
            "Epoch 6/50, Episode 180/200, Loss: 102.31232452392578, Avg. Reward: -3.2705581188201904\n",
            "Epoch 6/50, Episode 190/200, Loss: 89.4207992553711, Avg. Reward: -2.941931962966919\n",
            "Epoch 6/50, Episode 200/200, Loss: 86.93775177001953, Avg. Reward: -2.854001522064209\n",
            "Epoch 7/50, Episode 10/200, Loss: 99.05829620361328, Avg. Reward: -3.226719379425049\n",
            "Epoch 7/50, Episode 20/200, Loss: 89.12640380859375, Avg. Reward: -2.9052464962005615\n",
            "Epoch 7/50, Episode 30/200, Loss: 95.39256286621094, Avg. Reward: -3.092033863067627\n",
            "Epoch 7/50, Episode 40/200, Loss: 94.21334838867188, Avg. Reward: -3.013671398162842\n",
            "Epoch 7/50, Episode 50/200, Loss: 88.04700469970703, Avg. Reward: -2.877070188522339\n",
            "Epoch 7/50, Episode 60/200, Loss: 94.30278778076172, Avg. Reward: -3.034759998321533\n",
            "Epoch 7/50, Episode 70/200, Loss: 79.56036376953125, Avg. Reward: -2.6680381298065186\n",
            "Epoch 7/50, Episode 80/200, Loss: 85.18181610107422, Avg. Reward: -2.7717864513397217\n",
            "Epoch 7/50, Episode 90/200, Loss: 84.74320220947266, Avg. Reward: -2.7990102767944336\n",
            "Epoch 7/50, Episode 100/200, Loss: 110.63924407958984, Avg. Reward: -3.5115513801574707\n",
            "Epoch 7/50, Episode 110/200, Loss: 93.2833023071289, Avg. Reward: -3.0629796981811523\n",
            "Epoch 7/50, Episode 120/200, Loss: 83.21116638183594, Avg. Reward: -2.6958062648773193\n",
            "Epoch 7/50, Episode 130/200, Loss: 86.08785247802734, Avg. Reward: -2.912471294403076\n",
            "Epoch 7/50, Episode 140/200, Loss: 88.5877456665039, Avg. Reward: -2.925367832183838\n",
            "Epoch 7/50, Episode 150/200, Loss: 97.329345703125, Avg. Reward: -3.1656992435455322\n",
            "Epoch 7/50, Episode 160/200, Loss: 105.05712890625, Avg. Reward: -3.3574156761169434\n",
            "Epoch 7/50, Episode 170/200, Loss: 100.77192687988281, Avg. Reward: -3.3599789142608643\n",
            "Epoch 7/50, Episode 180/200, Loss: 97.74150085449219, Avg. Reward: -3.2124836444854736\n",
            "Epoch 7/50, Episode 190/200, Loss: 96.0650863647461, Avg. Reward: -3.070619821548462\n",
            "Epoch 7/50, Episode 200/200, Loss: 87.59562683105469, Avg. Reward: -2.839553117752075\n",
            "Epoch 8/50, Episode 10/200, Loss: 94.67578887939453, Avg. Reward: -3.039698600769043\n",
            "Epoch 8/50, Episode 20/200, Loss: 92.76130676269531, Avg. Reward: -3.045031785964966\n",
            "Epoch 8/50, Episode 30/200, Loss: 94.95840454101562, Avg. Reward: -3.1293249130249023\n",
            "Epoch 8/50, Episode 40/200, Loss: 90.02014923095703, Avg. Reward: -2.9485363960266113\n",
            "Epoch 8/50, Episode 50/200, Loss: 82.53276062011719, Avg. Reward: -2.702307939529419\n",
            "Epoch 8/50, Episode 60/200, Loss: 91.06904602050781, Avg. Reward: -3.016976833343506\n",
            "Epoch 8/50, Episode 70/200, Loss: 79.06258392333984, Avg. Reward: -2.5927670001983643\n",
            "Epoch 8/50, Episode 80/200, Loss: 76.23766326904297, Avg. Reward: -2.5581538677215576\n",
            "Epoch 8/50, Episode 90/200, Loss: 69.13887023925781, Avg. Reward: -2.273108959197998\n",
            "Epoch 8/50, Episode 100/200, Loss: 86.26751708984375, Avg. Reward: -2.770354747772217\n",
            "Epoch 8/50, Episode 110/200, Loss: 75.9652099609375, Avg. Reward: -2.505659580230713\n",
            "Epoch 8/50, Episode 120/200, Loss: 77.64938354492188, Avg. Reward: -2.532545328140259\n",
            "Epoch 8/50, Episode 130/200, Loss: 87.45799255371094, Avg. Reward: -2.7608306407928467\n",
            "Epoch 8/50, Episode 140/200, Loss: 79.96297454833984, Avg. Reward: -2.6385231018066406\n",
            "Epoch 8/50, Episode 150/200, Loss: 68.2897720336914, Avg. Reward: -2.2924273014068604\n",
            "Epoch 8/50, Episode 160/200, Loss: 66.16416931152344, Avg. Reward: -2.232651948928833\n",
            "Epoch 8/50, Episode 170/200, Loss: 78.51126861572266, Avg. Reward: -2.5180013179779053\n",
            "Epoch 8/50, Episode 180/200, Loss: 76.37401580810547, Avg. Reward: -2.5791079998016357\n",
            "Epoch 8/50, Episode 190/200, Loss: 79.7018051147461, Avg. Reward: -2.6130239963531494\n",
            "Epoch 8/50, Episode 200/200, Loss: 77.80442810058594, Avg. Reward: -2.583984613418579\n",
            "Epoch 9/50, Episode 10/200, Loss: 81.40679931640625, Avg. Reward: -2.783559799194336\n",
            "Epoch 9/50, Episode 20/200, Loss: 79.96499633789062, Avg. Reward: -2.7457430362701416\n",
            "Epoch 9/50, Episode 30/200, Loss: 85.07414245605469, Avg. Reward: -2.828648328781128\n",
            "Epoch 9/50, Episode 40/200, Loss: 88.11824798583984, Avg. Reward: -2.858029842376709\n",
            "Epoch 9/50, Episode 50/200, Loss: 80.17742156982422, Avg. Reward: -2.6790614128112793\n",
            "Epoch 9/50, Episode 60/200, Loss: 78.2623291015625, Avg. Reward: -2.664060592651367\n",
            "Epoch 9/50, Episode 70/200, Loss: 85.67345428466797, Avg. Reward: -2.8162877559661865\n",
            "Epoch 9/50, Episode 80/200, Loss: 82.82141876220703, Avg. Reward: -2.728621482849121\n",
            "Epoch 9/50, Episode 90/200, Loss: 88.3262939453125, Avg. Reward: -2.9291679859161377\n",
            "Epoch 9/50, Episode 100/200, Loss: 80.25888061523438, Avg. Reward: -2.636049509048462\n",
            "Epoch 9/50, Episode 110/200, Loss: 75.5731430053711, Avg. Reward: -2.5131869316101074\n",
            "Epoch 9/50, Episode 120/200, Loss: 96.21248626708984, Avg. Reward: -3.092144727706909\n",
            "Epoch 9/50, Episode 130/200, Loss: 84.79374694824219, Avg. Reward: -2.7821786403656006\n",
            "Epoch 9/50, Episode 140/200, Loss: 81.2955322265625, Avg. Reward: -2.7100162506103516\n",
            "Epoch 9/50, Episode 150/200, Loss: 88.81050109863281, Avg. Reward: -2.938182830810547\n",
            "Epoch 9/50, Episode 160/200, Loss: 80.77517700195312, Avg. Reward: -2.6734702587127686\n",
            "Epoch 9/50, Episode 170/200, Loss: 90.72207641601562, Avg. Reward: -2.953305959701538\n",
            "Epoch 9/50, Episode 180/200, Loss: 72.11930847167969, Avg. Reward: -2.385925054550171\n",
            "Epoch 9/50, Episode 190/200, Loss: 80.12532806396484, Avg. Reward: -2.679706573486328\n",
            "Epoch 9/50, Episode 200/200, Loss: 81.83397674560547, Avg. Reward: -2.7612409591674805\n",
            "Epoch 10/50, Episode 10/200, Loss: 83.43527221679688, Avg. Reward: -2.722531318664551\n",
            "Epoch 10/50, Episode 20/200, Loss: 92.43665313720703, Avg. Reward: -3.036303997039795\n",
            "Epoch 10/50, Episode 30/200, Loss: 86.06231689453125, Avg. Reward: -2.843480348587036\n",
            "Epoch 10/50, Episode 40/200, Loss: 80.55170440673828, Avg. Reward: -2.7606029510498047\n",
            "Epoch 10/50, Episode 50/200, Loss: 80.34380340576172, Avg. Reward: -2.693451404571533\n",
            "Epoch 10/50, Episode 60/200, Loss: 89.02693939208984, Avg. Reward: -2.9255623817443848\n",
            "Epoch 10/50, Episode 70/200, Loss: 95.77421569824219, Avg. Reward: -3.082960367202759\n",
            "Epoch 10/50, Episode 80/200, Loss: 83.3526382446289, Avg. Reward: -2.7521979808807373\n",
            "Epoch 10/50, Episode 90/200, Loss: 80.3017807006836, Avg. Reward: -2.722336769104004\n",
            "Epoch 10/50, Episode 100/200, Loss: 83.3957290649414, Avg. Reward: -2.727114200592041\n",
            "Epoch 10/50, Episode 110/200, Loss: 85.21826934814453, Avg. Reward: -2.7422337532043457\n",
            "Epoch 10/50, Episode 120/200, Loss: 87.10118103027344, Avg. Reward: -2.9052867889404297\n",
            "Epoch 10/50, Episode 130/200, Loss: 94.76488494873047, Avg. Reward: -3.0957889556884766\n",
            "Epoch 10/50, Episode 140/200, Loss: 78.10284423828125, Avg. Reward: -2.626720428466797\n",
            "Epoch 10/50, Episode 150/200, Loss: 79.9509506225586, Avg. Reward: -2.6938040256500244\n",
            "Epoch 10/50, Episode 160/200, Loss: 91.81417083740234, Avg. Reward: -3.0473926067352295\n",
            "Epoch 10/50, Episode 170/200, Loss: 86.84566497802734, Avg. Reward: -2.9129748344421387\n",
            "Epoch 10/50, Episode 180/200, Loss: 87.1781234741211, Avg. Reward: -2.8538613319396973\n",
            "Epoch 10/50, Episode 190/200, Loss: 93.72821807861328, Avg. Reward: -3.0825417041778564\n",
            "Epoch 10/50, Episode 200/200, Loss: 82.30068969726562, Avg. Reward: -2.7752044200897217\n",
            "Epoch 11/50, Episode 10/200, Loss: 84.34590911865234, Avg. Reward: -2.8850953578948975\n",
            "Epoch 11/50, Episode 20/200, Loss: 80.25209045410156, Avg. Reward: -2.698500633239746\n",
            "Epoch 11/50, Episode 30/200, Loss: 87.32882690429688, Avg. Reward: -2.911237955093384\n",
            "Epoch 11/50, Episode 40/200, Loss: 71.17717742919922, Avg. Reward: -2.353219747543335\n",
            "Epoch 11/50, Episode 50/200, Loss: 77.81208801269531, Avg. Reward: -2.623816728591919\n",
            "Epoch 11/50, Episode 60/200, Loss: 80.6175765991211, Avg. Reward: -2.704439640045166\n",
            "Epoch 11/50, Episode 70/200, Loss: 73.19224548339844, Avg. Reward: -2.44130802154541\n",
            "Epoch 11/50, Episode 80/200, Loss: 77.42340850830078, Avg. Reward: -2.5477921962738037\n",
            "Epoch 11/50, Episode 90/200, Loss: 79.19210815429688, Avg. Reward: -2.7586891651153564\n",
            "Epoch 11/50, Episode 100/200, Loss: 86.2852554321289, Avg. Reward: -2.7946321964263916\n",
            "Epoch 11/50, Episode 110/200, Loss: 81.13411712646484, Avg. Reward: -2.7581558227539062\n",
            "Epoch 11/50, Episode 120/200, Loss: 87.39209747314453, Avg. Reward: -2.8441457748413086\n",
            "Epoch 11/50, Episode 130/200, Loss: 98.5093765258789, Avg. Reward: -3.194190502166748\n",
            "Epoch 11/50, Episode 140/200, Loss: 82.69512176513672, Avg. Reward: -2.780858278274536\n",
            "Epoch 11/50, Episode 150/200, Loss: 72.79328918457031, Avg. Reward: -2.5352420806884766\n",
            "Epoch 11/50, Episode 160/200, Loss: 84.07108306884766, Avg. Reward: -2.78085994720459\n",
            "Epoch 11/50, Episode 170/200, Loss: 79.02107238769531, Avg. Reward: -2.6558921337127686\n",
            "Epoch 11/50, Episode 180/200, Loss: 84.25252532958984, Avg. Reward: -2.808563232421875\n",
            "Epoch 11/50, Episode 190/200, Loss: 83.29228973388672, Avg. Reward: -2.790820360183716\n",
            "Epoch 11/50, Episode 200/200, Loss: 73.57046508789062, Avg. Reward: -2.5203311443328857\n",
            "Epoch 12/50, Episode 10/200, Loss: 89.38929748535156, Avg. Reward: -2.963240623474121\n",
            "Epoch 12/50, Episode 20/200, Loss: 91.22744750976562, Avg. Reward: -3.0838091373443604\n",
            "Epoch 12/50, Episode 30/200, Loss: 75.4526138305664, Avg. Reward: -2.581986665725708\n",
            "Epoch 12/50, Episode 40/200, Loss: 88.33000946044922, Avg. Reward: -2.908750057220459\n",
            "Epoch 12/50, Episode 50/200, Loss: 83.35999298095703, Avg. Reward: -2.851210594177246\n",
            "Epoch 12/50, Episode 60/200, Loss: 76.89063262939453, Avg. Reward: -2.572370767593384\n",
            "Epoch 12/50, Episode 70/200, Loss: 90.90399169921875, Avg. Reward: -2.9515416622161865\n",
            "Epoch 12/50, Episode 80/200, Loss: 85.8223876953125, Avg. Reward: -2.849445343017578\n",
            "Epoch 12/50, Episode 90/200, Loss: 70.23617553710938, Avg. Reward: -2.4820947647094727\n",
            "Epoch 12/50, Episode 100/200, Loss: 76.87325286865234, Avg. Reward: -2.679509162902832\n",
            "Epoch 12/50, Episode 110/200, Loss: 89.7464599609375, Avg. Reward: -2.9150145053863525\n",
            "Epoch 12/50, Episode 120/200, Loss: 93.85830688476562, Avg. Reward: -3.121392011642456\n",
            "Epoch 12/50, Episode 130/200, Loss: 102.82398986816406, Avg. Reward: -3.370957612991333\n",
            "Epoch 12/50, Episode 140/200, Loss: 96.67604064941406, Avg. Reward: -3.208937168121338\n",
            "Epoch 12/50, Episode 150/200, Loss: 83.95874786376953, Avg. Reward: -2.827925682067871\n",
            "Epoch 12/50, Episode 160/200, Loss: 76.82423400878906, Avg. Reward: -2.6884920597076416\n",
            "Epoch 12/50, Episode 170/200, Loss: 78.71744537353516, Avg. Reward: -2.6802122592926025\n",
            "Epoch 12/50, Episode 180/200, Loss: 72.57337188720703, Avg. Reward: -2.4676504135131836\n",
            "Epoch 12/50, Episode 190/200, Loss: 80.244140625, Avg. Reward: -2.6820924282073975\n",
            "Epoch 12/50, Episode 200/200, Loss: 69.51663970947266, Avg. Reward: -2.3240203857421875\n",
            "Epoch 13/50, Episode 10/200, Loss: 70.27371215820312, Avg. Reward: -2.4423539638519287\n",
            "Epoch 13/50, Episode 20/200, Loss: 76.52397155761719, Avg. Reward: -2.642536163330078\n",
            "Epoch 13/50, Episode 30/200, Loss: 68.13087463378906, Avg. Reward: -2.3722360134124756\n",
            "Epoch 13/50, Episode 40/200, Loss: 85.21968078613281, Avg. Reward: -2.784349203109741\n",
            "Epoch 13/50, Episode 50/200, Loss: 91.94770050048828, Avg. Reward: -3.0044705867767334\n",
            "Epoch 13/50, Episode 60/200, Loss: 76.12116241455078, Avg. Reward: -2.576173782348633\n",
            "Epoch 13/50, Episode 70/200, Loss: 74.04878234863281, Avg. Reward: -2.5299205780029297\n",
            "Epoch 13/50, Episode 80/200, Loss: 73.0841064453125, Avg. Reward: -2.4825589656829834\n",
            "Epoch 13/50, Episode 90/200, Loss: 84.18936157226562, Avg. Reward: -2.851921796798706\n",
            "Epoch 13/50, Episode 100/200, Loss: 73.97416687011719, Avg. Reward: -2.583951711654663\n",
            "Epoch 13/50, Episode 110/200, Loss: 91.31497955322266, Avg. Reward: -3.0278899669647217\n",
            "Epoch 13/50, Episode 120/200, Loss: 85.23284912109375, Avg. Reward: -2.865872621536255\n",
            "Epoch 13/50, Episode 130/200, Loss: 87.9115219116211, Avg. Reward: -2.896947145462036\n",
            "Epoch 13/50, Episode 140/200, Loss: 80.7020492553711, Avg. Reward: -2.8279993534088135\n",
            "Epoch 13/50, Episode 150/200, Loss: 70.0419921875, Avg. Reward: -2.407287359237671\n",
            "Epoch 13/50, Episode 160/200, Loss: 81.01641845703125, Avg. Reward: -2.7781834602355957\n",
            "Epoch 13/50, Episode 170/200, Loss: 84.82599639892578, Avg. Reward: -2.8433010578155518\n",
            "Epoch 13/50, Episode 180/200, Loss: 82.13367462158203, Avg. Reward: -2.680959701538086\n",
            "Epoch 13/50, Episode 190/200, Loss: 86.63247680664062, Avg. Reward: -2.904587745666504\n",
            "Epoch 13/50, Episode 200/200, Loss: 84.32593536376953, Avg. Reward: -2.873911142349243\n",
            "Epoch 14/50, Episode 10/200, Loss: 79.11238098144531, Avg. Reward: -2.6760988235473633\n",
            "Epoch 14/50, Episode 20/200, Loss: 91.3830795288086, Avg. Reward: -3.0624196529388428\n",
            "Epoch 14/50, Episode 30/200, Loss: 88.5617904663086, Avg. Reward: -2.924464702606201\n",
            "Epoch 14/50, Episode 40/200, Loss: 90.90008544921875, Avg. Reward: -3.031061887741089\n",
            "Epoch 14/50, Episode 50/200, Loss: 80.87848663330078, Avg. Reward: -2.725149154663086\n",
            "Epoch 14/50, Episode 60/200, Loss: 80.48056030273438, Avg. Reward: -2.716750383377075\n",
            "Epoch 14/50, Episode 70/200, Loss: 75.2151107788086, Avg. Reward: -2.6337900161743164\n",
            "Epoch 14/50, Episode 80/200, Loss: 93.91403198242188, Avg. Reward: -3.1304471492767334\n",
            "Epoch 14/50, Episode 90/200, Loss: 83.16018676757812, Avg. Reward: -2.8191652297973633\n",
            "Epoch 14/50, Episode 100/200, Loss: 91.32420349121094, Avg. Reward: -3.0493013858795166\n",
            "Epoch 14/50, Episode 110/200, Loss: 89.41357421875, Avg. Reward: -3.0233585834503174\n",
            "Epoch 14/50, Episode 120/200, Loss: 78.91146850585938, Avg. Reward: -2.741252899169922\n",
            "Epoch 14/50, Episode 130/200, Loss: 88.16082763671875, Avg. Reward: -2.9767231941223145\n",
            "Epoch 14/50, Episode 140/200, Loss: 94.16386413574219, Avg. Reward: -3.2450480461120605\n",
            "Epoch 14/50, Episode 150/200, Loss: 102.66988372802734, Avg. Reward: -3.4376673698425293\n",
            "Epoch 14/50, Episode 160/200, Loss: 95.9834976196289, Avg. Reward: -3.193082094192505\n",
            "Epoch 14/50, Episode 170/200, Loss: 88.69916534423828, Avg. Reward: -3.0728235244750977\n",
            "Epoch 14/50, Episode 180/200, Loss: 81.31096649169922, Avg. Reward: -2.7938053607940674\n",
            "Epoch 14/50, Episode 190/200, Loss: 89.96725463867188, Avg. Reward: -3.0081217288970947\n",
            "Epoch 14/50, Episode 200/200, Loss: 80.42503356933594, Avg. Reward: -2.862837314605713\n",
            "Epoch 15/50, Episode 10/200, Loss: 78.69528198242188, Avg. Reward: -2.6431572437286377\n",
            "Epoch 15/50, Episode 20/200, Loss: 85.12611389160156, Avg. Reward: -2.905031442642212\n",
            "Epoch 15/50, Episode 30/200, Loss: 71.13887786865234, Avg. Reward: -2.5769100189208984\n",
            "Epoch 15/50, Episode 40/200, Loss: 82.41898345947266, Avg. Reward: -2.908223867416382\n",
            "Epoch 15/50, Episode 50/200, Loss: 71.45330047607422, Avg. Reward: -2.412642240524292\n",
            "Epoch 15/50, Episode 60/200, Loss: 65.1572265625, Avg. Reward: -2.3381357192993164\n",
            "Epoch 15/50, Episode 70/200, Loss: 67.52349853515625, Avg. Reward: -2.29683780670166\n",
            "Epoch 15/50, Episode 80/200, Loss: 70.6771011352539, Avg. Reward: -2.426614999771118\n",
            "Epoch 15/50, Episode 90/200, Loss: 68.57305145263672, Avg. Reward: -2.3911068439483643\n",
            "Epoch 15/50, Episode 100/200, Loss: 62.68828201293945, Avg. Reward: -2.301811933517456\n",
            "Epoch 15/50, Episode 110/200, Loss: 72.11690521240234, Avg. Reward: -2.541944980621338\n",
            "Epoch 15/50, Episode 120/200, Loss: 67.41246795654297, Avg. Reward: -2.50343656539917\n",
            "Epoch 15/50, Episode 130/200, Loss: 68.54998779296875, Avg. Reward: -2.356940507888794\n",
            "Epoch 15/50, Episode 140/200, Loss: 74.94888305664062, Avg. Reward: -2.596167802810669\n",
            "Epoch 15/50, Episode 150/200, Loss: 69.06106567382812, Avg. Reward: -2.4470338821411133\n",
            "Epoch 15/50, Episode 160/200, Loss: 72.1117172241211, Avg. Reward: -2.4774832725524902\n",
            "Epoch 15/50, Episode 170/200, Loss: 76.23904418945312, Avg. Reward: -2.6142024993896484\n",
            "Epoch 15/50, Episode 180/200, Loss: 74.19466400146484, Avg. Reward: -2.602940082550049\n",
            "Epoch 15/50, Episode 190/200, Loss: 65.2524642944336, Avg. Reward: -2.2900633811950684\n",
            "Epoch 15/50, Episode 200/200, Loss: 71.93720245361328, Avg. Reward: -2.482199192047119\n",
            "Epoch 16/50, Episode 10/200, Loss: 67.93396759033203, Avg. Reward: -2.3862783908843994\n",
            "Epoch 16/50, Episode 20/200, Loss: 88.24783325195312, Avg. Reward: -2.9733920097351074\n",
            "Epoch 16/50, Episode 30/200, Loss: 84.46443176269531, Avg. Reward: -2.8812882900238037\n",
            "Epoch 16/50, Episode 40/200, Loss: 80.25240325927734, Avg. Reward: -2.7161083221435547\n",
            "Epoch 16/50, Episode 50/200, Loss: 81.27640533447266, Avg. Reward: -2.8396947383880615\n",
            "Epoch 16/50, Episode 60/200, Loss: 71.92536926269531, Avg. Reward: -2.571868419647217\n",
            "Epoch 16/50, Episode 70/200, Loss: 79.3167495727539, Avg. Reward: -2.8091259002685547\n",
            "Epoch 16/50, Episode 80/200, Loss: 84.25311279296875, Avg. Reward: -3.008572816848755\n",
            "Epoch 16/50, Episode 90/200, Loss: 82.97859954833984, Avg. Reward: -2.9161295890808105\n",
            "Epoch 16/50, Episode 100/200, Loss: 88.40910339355469, Avg. Reward: -3.0648248195648193\n",
            "Epoch 16/50, Episode 110/200, Loss: 83.94866943359375, Avg. Reward: -2.935321569442749\n",
            "Epoch 16/50, Episode 120/200, Loss: 95.79239654541016, Avg. Reward: -3.309727191925049\n",
            "Epoch 16/50, Episode 130/200, Loss: 93.75077819824219, Avg. Reward: -3.1586835384368896\n",
            "Epoch 16/50, Episode 140/200, Loss: 87.3557357788086, Avg. Reward: -2.914045572280884\n",
            "Epoch 16/50, Episode 150/200, Loss: 68.4190444946289, Avg. Reward: -2.469484567642212\n",
            "Epoch 16/50, Episode 160/200, Loss: 81.97761535644531, Avg. Reward: -2.9209160804748535\n",
            "Epoch 16/50, Episode 170/200, Loss: 87.11170959472656, Avg. Reward: -2.8888661861419678\n",
            "Epoch 16/50, Episode 180/200, Loss: 79.7628173828125, Avg. Reward: -2.7110817432403564\n",
            "Epoch 16/50, Episode 190/200, Loss: 87.25942993164062, Avg. Reward: -2.9823925495147705\n",
            "Epoch 16/50, Episode 200/200, Loss: 96.87518310546875, Avg. Reward: -3.2652645111083984\n",
            "Epoch 17/50, Episode 10/200, Loss: 82.61551666259766, Avg. Reward: -2.9219517707824707\n",
            "Epoch 17/50, Episode 20/200, Loss: 77.91890716552734, Avg. Reward: -2.7204089164733887\n",
            "Epoch 17/50, Episode 30/200, Loss: 76.51826477050781, Avg. Reward: -2.671293020248413\n",
            "Epoch 17/50, Episode 40/200, Loss: 92.61994171142578, Avg. Reward: -3.0681138038635254\n",
            "Epoch 17/50, Episode 50/200, Loss: 80.79000854492188, Avg. Reward: -2.717822313308716\n",
            "Epoch 17/50, Episode 60/200, Loss: 91.00626373291016, Avg. Reward: -3.2105319499969482\n",
            "Epoch 17/50, Episode 70/200, Loss: 86.141357421875, Avg. Reward: -2.9193270206451416\n",
            "Epoch 17/50, Episode 80/200, Loss: 78.58269500732422, Avg. Reward: -2.7168941497802734\n",
            "Epoch 17/50, Episode 90/200, Loss: 78.34525299072266, Avg. Reward: -2.7723193168640137\n",
            "Epoch 17/50, Episode 100/200, Loss: 75.18022918701172, Avg. Reward: -2.6353421211242676\n",
            "Epoch 17/50, Episode 110/200, Loss: 85.02155303955078, Avg. Reward: -2.9113004207611084\n",
            "Epoch 17/50, Episode 120/200, Loss: 82.46697235107422, Avg. Reward: -2.957458257675171\n",
            "Epoch 17/50, Episode 130/200, Loss: 68.71788024902344, Avg. Reward: -2.5009162425994873\n",
            "Epoch 17/50, Episode 140/200, Loss: 73.3711166381836, Avg. Reward: -2.593301296234131\n",
            "Epoch 17/50, Episode 150/200, Loss: 66.41079711914062, Avg. Reward: -2.35029673576355\n",
            "Epoch 17/50, Episode 160/200, Loss: 69.2981948852539, Avg. Reward: -2.477954387664795\n",
            "Epoch 17/50, Episode 170/200, Loss: 75.84596252441406, Avg. Reward: -2.747633695602417\n",
            "Epoch 17/50, Episode 180/200, Loss: 69.84710693359375, Avg. Reward: -2.562687873840332\n",
            "Epoch 17/50, Episode 190/200, Loss: 71.2655258178711, Avg. Reward: -2.548798084259033\n",
            "Epoch 17/50, Episode 200/200, Loss: 71.32601928710938, Avg. Reward: -2.5623059272766113\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for episode in range(num_episodes):\n",
        "        env.reset(seed=epoch + 1000)\n",
        "        collector = SyncDataCollector(\n",
        "            env,\n",
        "            model.get_policy_operator(),\n",
        "            frames_per_batch=frames_per_batch,\n",
        "            total_frames=frames_per_batch,\n",
        "            device=device,\n",
        "        )\n",
        "        replay_buffer = ReplayBuffer(\n",
        "            storage=LazyTensorStorage(max_size=frames_per_batch),\n",
        "            sampler=SamplerWithoutReplacement(),\n",
        "        )\n",
        "        for batch in collector:\n",
        "            advantage_module(batch)\n",
        "            replay_buffer.extend(batch.reshape(-1).cpu())\n",
        "            for _ in range(sub_batch_num):\n",
        "                subdata = replay_buffer.sample(sub_batch_size)\n",
        "                optim.zero_grad()\n",
        "                # Forward pass PPO loss\n",
        "                loss = loss_module(subdata.to(device))\n",
        "                loss_sum = (\n",
        "                    loss[\"loss_critic\"] + loss[\"loss_objective\"] + loss[\"loss_entropy\"]\n",
        "                )\n",
        "                # Backward pass\n",
        "                loss_sum.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(loss_module.parameters(), max_norm=1.0)\n",
        "                for param in loss_module.parameters():\n",
        "                    if param.grad is not None:\n",
        "                        param.grad = torch.nan_to_num(param.grad)\n",
        "                # Update the networks\n",
        "                optim.step()\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print(\n",
        "                f\"Epoch {epoch+1}/{num_epochs}, Episode {episode + 1}/{num_episodes}, Loss: {loss_sum.item()}, Avg. Reward: {batch['next', 'reward'].mean().item()}\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test\n",
        "\n",
        "base_env = HedgingEnv(S0, K, sigma, r, num_simulation=5, num_step=num_step, history_len=history_len)\n",
        "env = GymWrapper(base_env, device=device)\n",
        "env.reset(seed=0)\n",
        "\n",
        "with set_exploration_type(ExplorationType.DETERMINISTIC):\n",
        "    rollout = env.rollout(max_steps=num_step, policy=model.get_policy_operator())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards = rollout['next', 'reward'].detach().cpu().numpy()\n",
        "rewards.min(), rewards.max(), rewards.mean(), rewards.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_portfolio_vs_option_price(env._env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
